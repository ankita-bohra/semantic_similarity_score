{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Semantic Similarity Model - It will calculate the similarity score for two different sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model used -> sentence-transformers/bert-base-nli-mean-tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karan\\AppData\\Roaming\\Python\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the data from the .csv file and converting it to list, where sentence1 is having 'text1' column value & sentence2 is having 'text2' column values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Precily_Text_Similarity_sample.csv', encoding='utf-8')\n",
    "sentence1 = df.text1.tolist()\n",
    "sentence2 = df.text2.tolist()\n",
    "\n",
    "model_name = 'sentence-transformers/bert-base-nli-mean-tokens'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding the sentence1 and sentence2 list to Sentences list as such that the sentence2[0] comes after sentence1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "sentences = [item for sublist in zip(sentence1, sentence2) for item in sublist]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Beneath the starry night, the silhouettes of ancient ruins whispered tales of civilizations long forgotten.',\n",
       " 'My things are not good.']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing tokenizer & model with the hugging face pretrained 'sentence-transformers' and 'bert-base-nli-mean-tokens' respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizing all the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = {'input_ids': [], 'attention_mask': []}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterating tokenizer for all the input sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in sentences:\n",
    "    new_tokens = tokenizer.encode_plus(sentence, max_length=128,\n",
    "                                       truncation=True, padding='max_length', return_tensors='pt')\n",
    "    tokens['input_ids'].append(new_tokens['input_ids'][0])\n",
    "    tokens['attention_mask'].append(new_tokens['attention_mask'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stacking the tokens so that it will be converted to single pytorch tensor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens['input_ids'] = torch.stack(tokens['input_ids'])\n",
    "tokens['attention_mask'] = torch.stack(tokens['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 128])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens['input_ids'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passing the tokens to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(**tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 4.1199e-01,  7.7777e-01,  6.0520e-01,  ..., -2.0810e-01,\n",
       "           8.5562e-02,  2.4778e-01],\n",
       "         [ 7.9102e-01,  8.8493e-01,  3.5878e-01,  ...,  2.1151e-01,\n",
       "          -1.1117e-01, -2.0535e-01],\n",
       "         [ 6.9259e-01,  5.4534e-01,  9.6198e-01,  ...,  7.8976e-02,\n",
       "          -1.1591e-01, -1.5669e-01],\n",
       "         ...,\n",
       "         [ 2.7982e-01,  4.8222e-01,  5.7033e-01,  ...,  2.0525e-01,\n",
       "          -3.1677e-01, -1.2900e-01],\n",
       "         [ 2.7551e-01,  4.4563e-01,  4.4077e-01,  ...,  1.9826e-01,\n",
       "          -4.0210e-01, -2.3859e-02],\n",
       "         [ 3.7187e-01,  2.8694e-01,  3.8014e-01,  ...,  2.5909e-01,\n",
       "          -4.3329e-01,  5.5404e-02]],\n",
       "\n",
       "        [[ 6.3068e-01,  5.6814e-02,  1.8481e+00,  ..., -4.0533e-01,\n",
       "          -5.2693e-01, -5.2335e-02],\n",
       "         [ 7.0392e-01,  4.9431e-02,  1.9409e+00,  ..., -4.8916e-01,\n",
       "          -5.4688e-01,  1.1321e-01],\n",
       "         [ 4.5755e-01,  1.6717e-01,  1.3666e+00,  ..., -5.0511e-01,\n",
       "          -5.8847e-01, -5.1262e-02],\n",
       "         ...,\n",
       "         [ 7.1523e-01, -6.5875e-04,  1.4369e+00,  ..., -2.6867e-01,\n",
       "          -5.1816e-01, -1.0362e-02],\n",
       "         [ 7.0878e-01,  4.1411e-02,  1.3170e+00,  ..., -3.2108e-01,\n",
       "          -5.7722e-01,  2.9656e-02],\n",
       "         [ 6.4170e-01, -6.2931e-02,  1.4556e+00,  ..., -3.4838e-01,\n",
       "          -6.0705e-01,  1.7470e-02]]], grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-0.7629, -0.1497,  0.0631,  ..., -0.1567, -0.4665,  0.9347],\n",
       "        [-0.3906, -0.2981, -0.3253,  ..., -0.3584, -0.1103,  0.8277]],\n",
       "       grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['last_hidden_state', 'pooler_output'])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assigning embeddings last_hidden_state tensors so that after performing mean pooling operating convert it into the sentence vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 128, 768])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = outputs.last_hidden_state\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiplying the attention_mask (i.e., 0 for no word in a cell and 1 for a word in a cell) to each embeddings which will help in removing the unnecessary embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 128])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention = tokens['attention_mask']\n",
    "attention.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing the dimension of attention to match with embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = attention.unsqueeze(-1).expand(embeddings.shape).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_embeddings = embeddings * mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting mask_embeddings to a single value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 768])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summed = torch.sum(mask_embeddings, 1)\n",
    "summed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking the counts of mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 768])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = torch.clamp(mask.sum(1), min=1e-9)\n",
    "counts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the mean pooling of the masked embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 768])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_pooled = summed/counts\n",
    "mean_pooled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6129,  0.7202,  0.7687,  ...,  0.0462, -0.0825, -0.1133],\n",
       "        [ 0.5945,  0.0229,  1.7749,  ..., -0.4434, -0.6163, -0.2169]],\n",
       "       grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_pooled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By Cosine similarity method, comparing which vector is most likely to match each other. Here I took Sentence1 comparision to sentence2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity between text1 and text2 : 0.3213573694229126\n"
     ]
    }
   ],
   "source": [
    "# for i in range(len(mean_pooled)-1):\n",
    "mean_pooled = mean_pooled.detach().numpy()\n",
    "\n",
    "# cosine_similarity(\n",
    "#     [mean_pooled[0]],\n",
    "#     [mean_pooled[1]]\n",
    "#  )\n",
    " \n",
    "for i in range(0, len(mean_pooled)-1, 2):\n",
    "    similarity = cosine_similarity([mean_pooled[i]], [mean_pooled[i+1]])[0, 0]\n",
    "    print(f\"Cosine Similarity between text1 and text2 : {similarity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle_out = open(\"Cosine_Similarity.pkl\",\"wb\")\n",
    "pickle.dump(similarity, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
